---
title: Introduction to linear models
author: ''
date: '2021-01-02'
slug: delete
categories:
  - R
tags:
  - R Markdown
---



<div id="introduction-to-linear-models" class="section level1">
<h1>Introduction to linear models</h1>
<p>Linear regression is a very powerful statistical technique.
Many people have some familiarity with regression just from reading the news, where straight lines are overlaid on scatterplots.
Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables.</p>
<p>Residual Analysis in Linear Regression</p>
<p>Linear regression is a statistical method for for modelling the linear relationship between a dependent variable y (i.e. the one we want to predict) and one or more explanatory or independent variables(X).</p>
<p>This vignette (<a href="https://rpubs.com/iabrady/residual-analysis" class="uri">https://rpubs.com/iabrady/residual-analysis</a>) will explain how residual plots generated by the regression function can be used to validate that some of the assumptions that are made about the dataset indicating it is suitable for a linear regression are met.</p>
<p>If you have ever wondered what these mean and how they can help - this is a small guide!</p>
<p>There are a number of assupmtions we made about the data and these must be met for a linear model to work successfully and the standard residual plots can help validate some of these. These are:</p>
<p>The dataset must have some linear relationship
Multivariate normality - the dataset variables must be statistically Normally Distributed (i.e. resembling a Bell Curve)
It must have no or little multicollinearity - this means the independent variables must not be too highly correlated with each other. This can be tested with a Correlation matrix and other tests
No auto-correlation - Autocorrelation occurs when the residuals are not independent from each other. For instance, this typically occurs in stock prices, where the price is not independent from the previous price.
Homoscedasticity - meaning that the residuals are equally distributed across the regression line i.e. above and below the regression line and the variance of the residuals should be the same for all predicted scores along the regression line.</p>
<p>Four standard plots can be accessed using the plot() function with the fit variable once the model is generated. These can be used to show if there are problems with the dataset and the model produced that need to be considered in looking at the validity of the model. These are:</p>
<pre><code>Residuals vs Fitted Plot
Normal Q–Q (quantile-quantile) Plot
Scale-Location
Residuals vs Leverage</code></pre>
<p>The mtcars dataset is used as an example to show the residual plots. The dataset describes the attibutes of various cars and how these relate to the dependent variable mpg i.e. how to things like weight, no of cylinders and no of gears affect miles per gallon (mpg). For this example we will use mpg (mpg) vs weight (wt).</p>
<div id="fitting-a-line-residuals-and-correlation" class="section level2">
<h2>Fitting a line, residuals, and correlation</h2>
<p>It’s helpful to think deeply about the line fitting process.
In this section, we define the form of a linear model, explore criteria for what makes a good fit, and introduce a new statistic called <em>correlation</em>.</p>
<div id="fitting-a-line-to-data" class="section level3">
<h3>Fitting a line to data</h3>
<pre class="r"><code>data(mtcars)
head(mtcars)</code></pre>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<pre class="r"><code>plot(mtcars$wt,mtcars$mpg)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>library(ggplot2)
# Basic scatter plot
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" width="672" />
###Fitting the Regression Line and its Residuals</p>
<p>Using the mtcars dataset we can use the lm linear regression function to fit a regression line and then plot it to see the results. The plot shows a good looking regression line.</p>
<p>The plot shows graphically the size of the residual value using a colour code (red is longer line to green - smaller line) and size of point. The size of residual is the length of the vertical line from the point to where it meets the regression line.</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Based on this graph, what mpg would you predict for a car weighing 4.5 (lbs in 1,000’s)?</p>
<pre class="r"><code>data(&quot;women&quot;)
head(women)</code></pre>
<pre><code>##   height weight
## 1     58    115
## 2     59    117
## 3     60    120
## 4     61    123
## 5     62    126
## 6     63    129</code></pre>
<pre class="r"><code>plot(women$height,women$weight)
lm1 &lt;- lm(women$weight~women$height)
summary(lm1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = women$weight ~ women$height)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -87.51667    5.93694  -14.74 1.71e-09 ***
## women$height   3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,  Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14</code></pre>
<pre class="r"><code>abline(lm1)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>shows two variables whose relationship can be modeled nearly perfectly with a straight line.
The equation for the line is <span class="math inline">\(y = -87.51667 + 3.45000 x\)</span>.
Consider what a perfect linear relationship means: we know the exact value of <span class="math inline">\(y\)</span> just by knowing the value of <span class="math inline">\(x\)</span>.
Perfect fit is unrealistic in almost any natural process.
For example, if we took family income (<span class="math inline">\(x\)</span>), this value would provide some useful information about how much financial support a college may offer a prospective student (<span class="math inline">\(y\)</span>).
However, the prediction would be far from perfect, since other factors play a role in financial support beyond a family’s finances.</p>
<p>Linear regression is the statistical method for fitting a line to data where the relationship between two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, can be modeled by a straight line with some error:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1x + \varepsilon\]</span></p>
<p>The values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent the model’s parameters (<span class="math inline">\(\beta\)</span> is the Greek letter <em>beta</em>), and the error is represented by <span class="math inline">\(\varepsilon\)</span> (the Greek letter <em>epsilon</em>).
The parameters are estimated using data, and we write their point estimates as <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>.
When we use <span class="math inline">\(x\)</span> to predict <span class="math inline">\(y\)</span>, we usually call <span class="math inline">\(x\)</span> the explanatory or <strong>predictor</strong> variable, and we call <span class="math inline">\(y\)</span> the response; we also often drop the <span class="math inline">\(\epsilon\)</span> term when writing down the model since our main focus is often on the prediction of the average outcome.</p>
<p>It is rare for all of the data to fall perfectly on a straight line.
Instead, it’s more common for data to appear as a <em>cloud of points</em>, such as those examples shown in Figure <a href="#fig:imperfLinearModel"><strong>??</strong></a>.
In each case, the data fall around a straight line, even if none of the observations fall exactly on the line.
The first plot shows a relatively strong downward linear trend, where the remaining variability in the data around the line is minor relative to the strength of the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.
The second plot shows an upward trend that, while evident, is not as strong as the first.
The last plot shows a very weak downward trend in the data, so slight we can hardly notice it.
In each of these examples, we will have some uncertainty regarding our estimates of the model parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
For instance, we might wonder, should we move the line up or down a little, or should we tilt it more or less?
As we move forward in this chapter, we will learn about criteria for line-fitting, and we will also learn about the uncertainty associated with estimates of model parameters.</p>
<p>There are also cases where fitting a straight line to the data, even if there is a clear relationship between the variables, is not helpful.
One such case is shown in Figure <a href="#fig:notGoodAtAllForALinearModel"><strong>??</strong></a> where there is a very clear relationship between the variables even though the trend is not linear.</p>
<pre class="r"><code>library(lattice)
par(mfrow=c(2,1)) 
vals&lt;-data.frame(x=1:10,y=(1:10)^2)
xyplot(x~y,data=vals)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>vals&lt;-data.frame(x=1:10,y=(1:10)^2)
xyplot(x~y,data=vals,type=c(&quot;p&quot;,&quot;smooth&quot;))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<p>We discuss nonlinear trends briefly here but details of fitting nonlinear models are saved for a later course.</p>
<pre class="marginfigure"><code>Now (?) might be a great time for us to check out this website: http://guessthecorrelation.com/</code></pre>
<p>Your homework due before class tomorrow is to watch these videos which are posted under the linear regression header on Brightspace and then do the following homework:
Videos to watch:
1. Linear regression women
2. Best fit line women</p>
<p>Once you have watched these videos, and you can refer to them as often as you would like, please answer and do the following:</p>
<ol style="list-style-type: decimal">
<li>Use linear regression to predict the weight of a woman who is 100 inches tall.</li>
<li>Use linear regression to predict the height of the woman who weighs 200 pounds.</li>
<li>Use linear regression to predict the height of a woman who weighs 5 pounds.</li>
<li>Use linear regression to predict the weight of a woman who is 200 inches tall.</li>
<li>Plot weight on the X axes and height on the y-axes and create a best fit line on your plot.</li>
<li>Plot height on the y-axes and wait on the X axes and create a best fit line on your plot.</li>
<li>Add a another column to the women dataframe called GPA which is these 15 numbers: 1.5,4,2,3.7,4,1, 3, 2.5, 3.8, 0.8, 2, 4, 1, 3, 2.</li>
<li>Use GPA to predict height. Is GPA a significant predictor and how do you know? Draw a best fit line on this relationship.</li>
<li>Use GPA to predict a weight. Is GPA a significant predictor and how do you know? Draw a bested line on this relationship, too.</li>
<li>Predict the height of a person with a GPA of 4.0.</li>
</ol>
</div>
</div>
</div>
